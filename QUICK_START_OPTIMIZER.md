# Token Optimizer å¿«é€Ÿé–‹å§‹æŒ‡å—

## 30 åˆ†é˜å¿«é€Ÿå¯¦ä½œ - ç­–ç•¥äºŒï¼šå·¥å…·è¼¸å‡ºéæ¿¾å™¨

> **ç‚ºä»€éº¼é¸é€™å€‹ç­–ç•¥ï¼Ÿ**
> - âœ… å¯¦ä½œæœ€ç°¡å–®ï¼ˆ1-2 å°æ™‚ï¼‰
> - âœ… æ•ˆæœæœ€å…¨é¢ï¼ˆé©ç”¨æ‰€æœ‰å·¥å…·ï¼‰
> - âœ… ç¯€çœæœ€æ˜é¡¯ï¼ˆ70-92% tokensï¼‰
> - âœ… ç¶­è­·æˆæœ¬æœ€ä½ï¼ˆå¹¾ä¹ç‚ºé›¶ï¼‰

---

## ç¬¬ä¸€æ­¥ï¼šå®‰è£ï¼ˆ1 åˆ†é˜ï¼‰

å·²ç¶“ç‚ºæ‚¨å»ºç«‹å¥½äº†ï¼æª”æ¡ˆåœ¨ï¼š`token_optimizer.py`

ç„¡éœ€å®‰è£ä»»ä½•é¡å¤–å¥—ä»¶ï¼Œåªä½¿ç”¨ Python æ¨™æº–åº«ã€‚

---

## ç¬¬äºŒæ­¥ï¼šåŸºæœ¬ä½¿ç”¨ï¼ˆ5 åˆ†é˜ï¼‰

### æœ€ç°¡å–®çš„ç”¨æ³•

```python
from token_optimizer import TokenOptimizer

# åˆå§‹åŒ–ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
optimizer = TokenOptimizer(threshold=1000)

# åŒ…è£ä»»ä½•å¯èƒ½è¿”å›å¤§é‡è³‡æ–™çš„å‡½å¼
def my_api_call(query):
    result = some_external_api(query)  # å¯èƒ½è¿”å› 15,000 tokens
    return optimizer.optimize(result, source="api_name")  # åªè¿”å› 300 tokens

# ä½¿ç”¨
output = my_api_call("search query")
```

å°±é€™éº¼ç°¡å–®ï¼

---

## ç¬¬ä¸‰æ­¥ï¼šæ•´åˆåˆ°ç¾æœ‰å°ˆæ¡ˆï¼ˆ10 åˆ†é˜ï¼‰

### æ–¹æ¡ˆ Aï¼šåŒ…è£å€‹åˆ¥å·¥å…·

```python
from token_optimizer import TokenOptimizer

optimizer = TokenOptimizer()

# åŸå§‹ç¨‹å¼ç¢¼
def search_web(query):
    result = tavily_api.search(query)
    return result  # è¿”å› 15,000 tokens âŒ

# å„ªåŒ–å¾Œ
def search_web(query):
    result = tavily_api.search(query)
    return optimizer.optimize(result, source="web_search")  # è¿”å› 300 tokens âœ…
```

### æ–¹æ¡ˆ Bï¼šå…¨åŸŸåŒ…è£å™¨ï¼ˆå»ºè­°ï¼‰

```python
from token_optimizer import TokenOptimizer

# åˆå§‹åŒ–å…¨åŸŸå„ªåŒ–å™¨
global_optimizer = TokenOptimizer(
    threshold=1000,
    auto_cleanup=True,
    max_cache_age_days=7
)

# å»ºç«‹é€šç”¨åŒ…è£å™¨
def optimize_tool_output(tool_name):
    """è£é£¾å™¨ï¼šè‡ªå‹•å„ªåŒ–å·¥å…·è¼¸å‡º"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            return global_optimizer.optimize(result, source=tool_name)
        return wrapper
    return decorator

# ä½¿ç”¨è£é£¾å™¨
@optimize_tool_output("web_search")
def search_web(query):
    return tavily_api.search(query)

@optimize_tool_output("database")
def query_database(sql):
    return db.execute(sql).fetchall()

@optimize_tool_output("file_read")
def read_file(filepath):
    with open(filepath) as f:
        return f.read()
```

---

## ç¬¬å››æ­¥ï¼šé©—è­‰æ•ˆæœï¼ˆ5 åˆ†é˜ï¼‰

### æ¸¬è©¦è…³æœ¬

```python
from token_optimizer import TokenOptimizer

# åˆå§‹åŒ–
optimizer = TokenOptimizer(threshold=1000)

# æ¨¡æ“¬å¤§å‹è³‡æ–™
large_data = "x" * 50000  # 50,000 å­—å…ƒ

# å„ªåŒ–å‰
print(f"åŸå§‹å¤§å°ï¼š{len(large_data)} å­—å…ƒ")
print(f"ä¼°è¨ˆ tokensï¼š{len(large_data) // 4}")

# å„ªåŒ–å¾Œ
result = optimizer.optimize(large_data, source="test")
print(f"\nå„ªåŒ–å¾Œè¿”å›ï¼š")
print(f"é¡å‹ï¼š{result['type']}")
print(f"æª”æ¡ˆï¼š{result['file']}")
print(f"ç¯€çœ tokensï¼š{result['estimated_tokens_saved']:,}")

# æŸ¥çœ‹çµ±è¨ˆ
optimizer.print_stats()
```

### é æœŸè¼¸å‡º

```
åŸå§‹å¤§å°ï¼š50,000 å­—å…ƒ
ä¼°è¨ˆ tokensï¼š12,500

å„ªåŒ–å¾Œè¿”å›ï¼š
é¡å‹ï¼šfile_reference
æª”æ¡ˆï¼štemp/tool_outputs/test_20251124_143022_a3f2b5c1.txt
ç¯€çœ tokensï¼š12,425

============================================================
Token Optimizer çµ±è¨ˆ
============================================================
ç¸½å‘¼å«æ¬¡æ•¸ï¼š1
å„ªåŒ–æ¬¡æ•¸ï¼š1
ç›´æ¥è¿”å›ï¼š0
å„ªåŒ–ç‡ï¼š100.0%
ç¯€çœ tokensï¼š12,425
ä¼°è¨ˆç¯€çœæˆæœ¬ï¼š$0.37 (åŸºæ–¼ GPT-4 å®šåƒ¹)
============================================================
```

---

## å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šOderWhat å°ˆæ¡ˆ - æ•´åˆåˆ° AI Agent

```python
# åœ¨ agent/ai_dining_agent.py ä¸­

from token_optimizer import TokenOptimizer

class AIDiningAgent:
    def __init__(self):
        # åŠ å…¥å„ªåŒ–å™¨
        self.optimizer = TokenOptimizer(
            threshold=1000,
            cache_dir="temp/agent_outputs"
        )

    def search_restaurants(self, query):
        """æœå°‹é¤å»³ï¼ˆå¯èƒ½è¿”å›å¤§é‡è³‡æ–™ï¼‰"""
        results = self.google_maps_api.search(query)

        # è‡ªå‹•å„ªåŒ–å¤§å‹çµæœ
        return self.optimizer.optimize(
            results,
            source="restaurant_search"
        )

    def analyze_menu(self, restaurant_id):
        """åˆ†æèœå–®ï¼ˆå¯èƒ½å¾ˆé•·ï¼‰"""
        menu_data = self.fetch_menu(restaurant_id)

        # è‡ªå‹•å„ªåŒ–
        return self.optimizer.optimize(
            menu_data,
            source="menu_analysis"
        )

    def get_reviews(self, restaurant_id):
        """å–å¾—è©•è«–ï¼ˆé€šå¸¸å¾ˆå¤šï¼‰"""
        reviews = self.review_api.get_reviews(restaurant_id)

        # è‡ªå‹•å„ªåŒ–
        return self.optimizer.optimize(
            reviews,
            source="reviews"
        )
```

### ç¯„ä¾‹ 2ï¼šè³‡æ–™åº«æŸ¥è©¢å„ªåŒ–

```python
from token_optimizer import TokenOptimizer

optimizer = TokenOptimizer()

def get_user_history(user_id):
    """å–å¾—ä½¿ç”¨è€…å®Œæ•´æ­·å²ï¼ˆå¯èƒ½æœ‰æ•¸åƒç­†ï¼‰"""
    # åŸå§‹æŸ¥è©¢
    query = f"SELECT * FROM orders WHERE user_id = {user_id}"
    results = db.execute(query).fetchall()

    # å„ªåŒ–å¤§å‹çµæœé›†
    return optimizer.optimize(
        results,
        source="user_history"
    )
```

### ç¯„ä¾‹ 3ï¼šæ—¥èªŒåˆ†æå„ªåŒ–

```python
from token_optimizer import TokenOptimizer

optimizer = TokenOptimizer()

def analyze_error_logs(days=7):
    """åˆ†æéŒ¯èª¤æ—¥èªŒï¼ˆé€šå¸¸å¾ˆå¤§ï¼‰"""
    # è®€å–æ—¥èªŒ
    with open("logs/error.log") as f:
        logs = f.read()  # å¯èƒ½æœ‰ 100,000+ è¡Œ

    # å„ªåŒ–
    return optimizer.optimize(
        logs,
        source="error_logs"
    )
```

---

## é€²éšé…ç½®

### è‡ªè¨‚é–¾å€¼

```python
# ä¸åŒå·¥å…·ä½¿ç”¨ä¸åŒé–¾å€¼
api_optimizer = TokenOptimizer(threshold=500)    # API è¼ƒåš´æ ¼
file_optimizer = TokenOptimizer(threshold=2000)  # æª”æ¡ˆè¼ƒå¯¬é¬†

# æˆ–è€…å‹•æ…‹èª¿æ•´
def smart_optimize(content, content_type):
    thresholds = {
        "api": 500,
        "file": 2000,
        "database": 1000,
        "logs": 3000
    }
    optimizer = TokenOptimizer(threshold=thresholds.get(content_type, 1000))
    return optimizer.optimize(content, source=content_type)
```

### è‡ªå‹•æ¸…ç†èˆŠæª”æ¡ˆ

```python
# å•Ÿç”¨è‡ªå‹•æ¸…ç†
optimizer = TokenOptimizer(
    auto_cleanup=True,          # å•Ÿå‹•æ™‚æ¸…ç†
    max_cache_age_days=7        # ä¿ç•™ 7 å¤©
)

# æˆ–æ‰‹å‹•æ¸…ç†
optimizer._cleanup_old_files()
```

### å¼·åˆ¶å­˜æª”

```python
# å³ä½¿å…§å®¹å¾ˆå°ä¹Ÿè¦å­˜æª”ï¼ˆç”¨æ–¼éœ€è¦è¿½è¹¤çš„æƒ…æ³ï¼‰
result = optimizer.optimize(
    small_data,
    source="important",
    force_save=True
)
```

---

## ç›£æ§å’Œçµ±è¨ˆ

### å³æ™‚ç›£æ§

```python
optimizer = TokenOptimizer()

# ... åŸ·è¡Œä¸€å †æ“ä½œ ...

# éš¨æ™‚æŸ¥çœ‹çµ±è¨ˆ
stats = optimizer.get_stats()
print(f"å·²ç¯€çœ {stats['total_tokens_saved']:,} tokens")
print(f"é ä¼°ç¯€çœ {stats['estimated_cost_saved_usd']}")
```

### å®šæœŸå ±å‘Š

```python
import schedule
import time

def print_daily_stats():
    """æ¯å¤©åˆ—å°çµ±è¨ˆå ±å‘Š"""
    optimizer.print_stats()

# æ¯å¤©æ™šä¸Š 11:59 åˆ—å°
schedule.every().day.at("23:59").do(print_daily_stats)

while True:
    schedule.run_pending()
    time.sleep(60)
```

---

## å¸¸è¦‹å•é¡Œ

### Q1: å¦‚æœ LLM éœ€è¦å®Œæ•´å…§å®¹æ€éº¼è¾¦ï¼Ÿ

**A**: LLM å¯ä»¥ä½¿ç”¨è¿”å›çš„æª”æ¡ˆè·¯å¾‘ï¼Œç”¨ `grep` æˆ– `cat` æŸ¥çœ‹ï¼š

```python
# å„ªåŒ–å™¨è¿”å›
{
    "file": "temp/tool_outputs/search_20251124.txt",
    "instructions": [
        "å®Œæ•´å…§å®¹ï¼šcat temp/tool_outputs/search_20251124.txt",
        "æœå°‹ï¼šgrep 'keyword' temp/tool_outputs/search_20251124.txt"
    ]
}

# LLM åŸ·è¡Œ
# grep 'restaurant' temp/tool_outputs/search_20251124.txt
```

### Q2: æœƒä¸æœƒå½±éŸ¿æ•ˆèƒ½ï¼Ÿ

**A**: å¹¾ä¹æ²’æœ‰å½±éŸ¿ï¼š

- å¯«å…¥æª”æ¡ˆï¼š< 10ms
- ç¯€çœçš„ LLM è™•ç†æ™‚é–“ï¼š> 100ms
- æ·¨æ•ˆç›Šï¼š**æ›´å¿«** âœ…

### Q3: ç£ç¢Ÿç©ºé–“æœƒä¸æœƒçˆ†æ‰ï¼Ÿ

**A**: ä¸æœƒï¼š

- å•Ÿç”¨ `auto_cleanup=True`
- è¨­å®š `max_cache_age_days=7`
- 7 å¤©å¾Œè‡ªå‹•åˆªé™¤èˆŠæª”æ¡ˆ

ä¸€èˆ¬ä½¿ç”¨æƒ…æ³ä¸‹ï¼Œå¿«å–ç›®éŒ„ < 100MBã€‚

### Q4: å¦‚ä½•èˆ‡ç¾æœ‰ LLM æ¡†æ¶æ•´åˆï¼Ÿ

**A**: éå¸¸ç°¡å–®ï¼Œä»¥ LangChain ç‚ºä¾‹ï¼š

```python
from langchain.tools import Tool
from token_optimizer import TokenOptimizer

optimizer = TokenOptimizer()

# åŒ…è£å·¥å…·
def search_wrapper(query):
    result = original_search_tool(query)
    return optimizer.optimize(result, source="search")

# è¨»å†Šåˆ° LangChain
search_tool = Tool(
    name="search",
    func=search_wrapper,
    description="Search the web"
)
```

---

## æ•ˆç›Šä¼°ç®—

### æ‚¨çš„å°ˆæ¡ˆå ´æ™¯

å‡è¨­ OderWhat å°ˆæ¡ˆï¼š

```
æ¯æ—¥ä½¿ç”¨æƒ…æ³ï¼š
- é¤å»³æœå°‹ï¼š50 æ¬¡ Ã— 8,000 tokens = 400,000 tokens
- èœå–®åˆ†æï¼š30 æ¬¡ Ã— 5,000 tokens = 150,000 tokens
- è©•è«–æ“·å–ï¼š40 æ¬¡ Ã— 6,000 tokens = 240,000 tokens

æ¯æ—¥ç¸½è¨ˆï¼š790,000 tokens
æœˆç¸½è¨ˆï¼š23,700,000 tokens

åŸå§‹æˆæœ¬ï¼ˆGPT-4ï¼‰ï¼š
23.7M Ã— $0.03/1K = $711/æœˆ

ä½¿ç”¨å„ªåŒ–å™¨å¾Œï¼ˆç¯€çœ 85%ï¼‰ï¼š
3.555M Ã— $0.03/1K = $106.65/æœˆ

æ¯æœˆç¯€çœï¼š$604.35
å¹´åº¦ç¯€çœï¼š$7,252
```

### æŠ•è³‡å ±é…¬ç‡

```
å¯¦ä½œæ™‚é–“ï¼š1-2 å°æ™‚
å¹´åº¦ç¯€çœï¼š$7,252
æ™‚è–ªåƒ¹å€¼ï¼š$3,626 - $7,252 ğŸ’°
```

---

## ä¸‹ä¸€æ­¥

### âœ… ç«‹å³è¡Œå‹•ï¼ˆä»Šå¤©ï¼‰

1. è¤‡è£½ `token_optimizer.py` åˆ°å°ˆæ¡ˆ
2. æ‰¾å‡º 3 å€‹æœ€å¸¸å‘¼å«çš„å·¥å…·
3. ç”¨ `optimizer.optimize()` åŒ…è£å®ƒå€‘
4. åŸ·è¡Œæ¸¬è©¦ï¼ŒæŸ¥çœ‹çµ±è¨ˆ

### ğŸ¯ æœ¬é€±ç›®æ¨™

1. æ•´åˆåˆ°æ‰€æœ‰å¤–éƒ¨ API å‘¼å«
2. åŒ…è£è³‡æ–™åº«æŸ¥è©¢
3. å„ªåŒ–æª”æ¡ˆè®€å–æ“ä½œ
4. ç›£æ§ç¯€çœæ•ˆæœ

### ğŸš€ ä¸‹å€‹æœˆ

è€ƒæ…®å¯¦ä½œå…¶ä»–ç­–ç•¥ï¼š
- ç­–ç•¥ 1ï¼šAPI å¿«å–ï¼ˆç–ŠåŠ æ•ˆæœï¼‰
- ç­–ç•¥ 3ï¼šæ¼¸é€²å¼æª”æ¡ˆè®€å–ï¼ˆé€²ä¸€æ­¥å„ªåŒ–ï¼‰

---

## ç¸½çµ

**ç­–ç•¥äºŒï¼šå·¥å…·è¼¸å‡ºéæ¿¾å™¨** æ˜¯æœ€ä½³çš„èµ·é»ï¼Œå› ç‚ºï¼š

| ç‰¹æ€§ | è©•åƒ¹ |
|------|------|
| å¯¦ä½œæ™‚é–“ | âš¡ 1-2 å°æ™‚ |
| é€šç”¨æ€§ | ğŸŒŸ é©ç”¨æ‰€æœ‰å·¥å…· |
| æ•ˆæœ | ğŸ”¥ 70-92% ç¯€çœ |
| ç¶­è­· | âœ… å¹¾ä¹ç‚ºé›¶ |
| è¤‡é›œåº¦ | ğŸ˜Š æ¥µç°¡å–® |
| ROI | ğŸ’° æ¥µé«˜ |

**ç¾åœ¨å°±é–‹å§‹ï¼Œ30 åˆ†é˜å¾Œçœ‹åˆ°æ•ˆæœï¼**

---

éœ€è¦å”åŠ©ï¼Ÿæª¢æŸ¥ `token_optimizer.py` ä¸­çš„ `example_usage()` å‡½å¼æŸ¥çœ‹å®Œæ•´ç¯„ä¾‹ã€‚
